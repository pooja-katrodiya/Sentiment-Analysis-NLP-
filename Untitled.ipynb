{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b276dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kruta\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\kruta\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "C:\\Users\\kruta\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e469b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# data = pd.read_csv(\"sentiment_data.csv\")\n",
    "data = pd.read_csv(\"Sentiment_Data.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e163e920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451332, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f69cb698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tweet        1\n",
       "Sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6491706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "null_count = data[\"Tweet\"].isnull().sum()\n",
    "if null_count > 0:\n",
    "    data[\"Tweet\"].fillna(\"\", inplace=True)\n",
    "\n",
    "print(data[\"Tweet\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb238a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tweet        object\n",
       "Sentiment    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce5d36db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"you're\", 'them', 'whom', 'yourselves', 'has', 'ours', 'up', 'further', 'once', 'own', 'didn', 'we', 'our', 'not', 't', 'd', 'this', 'doesn', \"shan't\", \"mightn't\", 'himself', 'these', 'ma', 'they', 'now', 'that', 'an', 'yours', 'same', \"won't\", 'against', 'above', 'you', 'ain', \"that'll\", 'both', 'which', \"don't\", 'he', 'here', 'few', \"doesn't\", 'itself', 'and', 'why', 'was', 'between', 're', 'to', \"should've\", \"didn't\", 'during', 'can', 'in', 'again', 'theirs', 'those', \"weren't\", 'do', 'it', 'more', 'shouldn', \"haven't\", 'each', 'hasn', 'll', 'been', \"shouldn't\", 'there', 'm', 'the', 'won', 'because', 'through', 'mustn', 'their', 'did', 'very', 'how', 'before', 'were', 'any', 'will', 'only', 'no', 'don', \"hadn't\", 'from', 'have', 'she', \"it's\", 'does', 'mightn', 'out', 'having', 'his', 'themselves', 'or', 'just', 'being', \"wasn't\", 'with', 'weren', 'had', \"couldn't\", 'about', \"you'll\", 'as', 'nor', 'some', 'be', 'what', \"you'd\", 'i', 'me', 'herself', 'under', 'of', 'couldn', 'off', 'is', 'when', 'o', 've', \"mustn't\", 'where', \"needn't\", 'into', 'wouldn', 'down', 'other', 'your', 'my', 'ourselves', 'while', 'such', 'isn', 'needn', 'yourself', 'who', 'by', 'its', 'am', 'after', 'her', \"isn't\", 'most', \"you've\", 'shan', \"she's\", 'too', 'a', 'than', \"wouldn't\", 'then', 'him', 'doing', 'until', 's', 'but', \"hasn't\", 'haven', 'myself', 'so', 'at', 'should', 'aren', \"aren't\", 'all', 'y', 'below', 'over', 'hers', 'hadn', 'if', 'wasn', 'on', 'for', 'are'}\n"
     ]
    }
   ],
   "source": [
    "#English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a7d24cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         w\n",
      "1         w\n",
      "2         w\n",
      "3         w\n",
      "4         w\n",
      "         ..\n",
      "451327     \n",
      "451328     \n",
      "451329     \n",
      "451330    w\n",
      "451331     \n",
      "Name: cleaned_text, Length: 451332, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Assuming 'stop_words' is a set of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Vectorized text processing\n",
    "def preprocess_text(text):\n",
    "    # Removing ASCII and emojis\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    \n",
    "    # Removing URLs\n",
    "    text = re.sub(r'https?\\S+', '', text)\n",
    "    \n",
    "    # Removing words starting with '#' or '@'\n",
    "    text = re.sub(r'@[^\\s]+|#\\w+', '', text)\n",
    "    \n",
    "    # Removing specific patterns like \"(...)\" and \"!!!\"\n",
    "    text = re.sub(r'\\....|!!!', '', text)\n",
    "    \n",
    "    # Removing stopwords\n",
    "    text = ' '.join(w for w in text.split() if w.lower() not in stop_words)\n",
    "    return text\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "data['cleaned_text'] = data['Tweet'].apply(preprocess_text)\n",
    "print(data['cleaned_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f3e3d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['w', 'w', 'w', ..., '', 'w', ''], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_tags(string):\n",
    "    removelist = \"\"\n",
    "    result = re.sub('','',string)          #remove HTML tags\n",
    "    result = re.sub('https://.*','',result)   #remove URLs\n",
    "    result = re.sub(r'[^w'+removelist+']', ' ',result)    #remove non-alphanumeric characters \n",
    "    result = result.lower()\n",
    "    return result\n",
    "data['Tweet']=data['Tweet'].apply(lambda cw : remove_tags(cw)) \n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "data1 = np.array(data['Tweet'])\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd0ff1e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tweet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19984\\2831173530.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Tweet' is not defined"
     ]
    }
   ],
   "source": [
    "print(Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6685544",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1896151126.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\kruta\\AppData\\Local\\Temp\\ipykernel_19984\\1896151126.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    for index, row in data.iterrows():-\u001b[0m\n\u001b[1;37m                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "decoded_texts_cleaned = []\n",
    "\n",
    "for index, row in data.iterrows():-\n",
    "    #Removing ASCII/ may be emojis too because emojis are in text format\n",
    "    t = str(row['Tweet']).encode(\"ascii\", \"ignore\")\n",
    "    decoded_text = t.decode()\n",
    "    \n",
    "    # Removing URLs that are  starting with both 'http' or 'https' using regular expressions\n",
    "    text_without_urls = re.sub(r'https?\\S+', '', decoded_text)\n",
    "    \n",
    "    # Removing words that are starting with '#' or '@' using regular expressions\n",
    "    text_without_tags = re.sub(r'@[^\\s]+|#\\w+', '', text_without_urls)\n",
    "    \n",
    "    # Removing specific patterns like \"(...)\" and \"!!!\"\n",
    "    decoded_text_without_patterns = re.sub(r'\\....|!!!', '', text_without_tags)\n",
    "    \n",
    "    # Removing stop words\n",
    "    decoded_text_without_stopwords = ' '.join(w for w in decoded_text_without_patterns.split() if w.lower() not in stop_words)\n",
    "    decoded_texts_cleaned.append(decoded_text_without_stopwords)\n",
    "    decoded_texts_cleaned1 = pd.DataFrame(decoded_texts_cleaned)\n",
    "print(decoded_texts_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0eb20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc10e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
